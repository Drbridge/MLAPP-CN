# MLAPP 读书笔记 - 10 离散图模型(Directed graphical models)(贝叶斯网络(Bayes nets))

> A Chinese Notes of MLAPP,MLAPP 中文笔记项目 
https://zhuanlan.zhihu.com/python-kivy

记笔记的人：[cycleuser](https://www.zhihu.com/people/cycleuser/activities)

2018年7月1日18:59:12

## 10.1 概论

>以简单方式对待复杂系统的原则,我基本知道两个:首先就是模块化原则,其次就是抽象原则.我是机器学习中计算概率的辩护者,因为我相信概率论对这两种原则都有深刻又有趣的实现方式,分别通过可分解性和平均.在我看来,尽可能充分利用这两种机制,是机器学习的前进方向.                      Michael Jordan, 1997 (转引自 (Frey 1998)).

假如我们观测多组相关变量,比如文档中的词汇,或者图像中的像素,再或基因片段上的基因.怎么能简洁地表示联合分布$p(x|\theta)$呢?利用这个分布,给定其他变量情况下,怎么能以合理规模的计算时间来推导一系列的变量呢?怎么通过适当规模的数据来学习得到这个分布的参数呢?这些问题就是概率建模(probabilistic modeling),推导(inference)和学习(learning)的核心,也是本章主题了.

### 10.1.1 链式规则(Chain rule)

通过概率论的链式规则,就可以讲一个联合分布写成下面的形式,使用任意次序的变量都可以:

$p(x_{1:V} ) = p(x_1)p(x_2|x_1)p(x_3|x_2, x_1)p(x_4|x_1, x_2, x_3) ... p(x_V |x_{1:V −1})$(10.1)

其中的V是变量数目,MATLAB风格的记号$1:V$表示集合{1,2,...,V},为了简洁,上面狮子中去掉了对固定参数$\theta$的条件.这个表达式的问题在于随着T变大,要表示条件分布$p(x_t|x_{1:t-1})$就越来越复杂了.

例如,设所有变量都有K个状态(states).然后可以将$p(x_1)$表示为$O(K)$个数字的表格,表示了一个离散分布(实际上只有K-1个自由参数,因为总和为一的约束条件,不过这里为了写起来简单就写成O(K)了).然后还可以将$p(x_2|x_1)$写成一个$O(K^2)$个数值的表格,写出$p(x_2=j|x_1=i)=T_{ij}$;T叫做随机矩阵(stochastic matrix)对$0\le T+{ij}\le 1$的所有条目以及所有列i满足约束条件$\sum_jT_{ij}=1$.与此类似,还可以将$p(x_3|x_1, x_2)$表示成一个有$O(K^3)$个数值的三维表格.这些表格就叫做条件概率表格(conditional probability tables,缩写为CPT).然后在我们这个模型里面就有$O(K^V)$个参数.要学习得到这么多参数需要的数据量就要多得可怕了.


















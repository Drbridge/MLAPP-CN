# MLAPP 读书笔记 - 11 混合模型(Mixture models)和期望最大化算法(EM algorithm)

> A Chinese Notes of MLAPP,MLAPP 中文笔记项目 
https://zhuanlan.zhihu.com/python-kivy

记笔记的人：[cycleuser](https://www.zhihu.com/people/cycleuser/activities)

2018年07月12日16:10:02

## 11.1 隐藏变量模型

第十章讲的是用于定义高维度联合概率分布的图模型.基本思想是通过增加两个变量在图之间的一条边来对二者的依赖关系进行建模.(通常这里的图表示了条件独立性,不过你懂的.)

另外一种方法就是假设所有观测到的变量都是相关的,因为他们都来自一个隐藏的共同诱因(cause).具有隐藏变量的模型也叫做隐藏变量模型(latent variable models,缩写为LVM,还是中文牛对不对?完全可以望文生义了).这一章会发现,这种模型就比没有隐藏变量的模型更难拟合.不过也有一些很显著的优势,主要有两方面.首先是隐藏变量模型(LVM)通常比直接在可见空间表示相关性的模型有更少的参数,这个如图11.1所示.如果所有节点(包括H)都是二值化的,而所有条件概率分布(CPD)都是可以列表的,那么图中左边的模型有17个自有参数,而右边的有59个自有参数.
另外,隐藏变量模型(LVM)中的隐藏变量可以能是起到瓶颈(bottleneck)的作用,可以计算对数据的一个压缩表达.这就形成了无监督学习的基础,后面会看到.图11.2展示了一些 通用的隐藏变量模型结构*LVM structures),可以用于这一目的.一般有L个隐藏变量$z_{i1},...,z_{il}$,D个可见变量,$x_{i1},...,x_{iD}$,通常$D\gg L$.如果有$L>1$,就是只看到了一个隐藏变量;这时候的$z_i$通常是离散的,就有一个一到多的映射.另外也可以有一个多到一的映射,表示对每个观测变量的不同概念因子或者诱因;这些模型都形成了概率矩阵因式分解(probabilistic matrix factorization)的基础,会在本书27.6.2讲到.最后,还可以有一对一映射,可以用来表示$z_i\right\arrow x_i$.可以让这两个都为向量值的,这就包含了所有其他表示了.根据选择的似然函数$p(x_i|z_i)$和先验$p(z_i)$的不同,可以生成各种不同的模型,如表11.1所示.



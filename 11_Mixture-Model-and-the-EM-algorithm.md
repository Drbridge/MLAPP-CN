
# MLAPP 读书笔记 

## 11 混合模型和期望最大化算法

> A Chinese Notes of MLAPP，MLAPP 中文笔记项目 
https://zhuanlan.zhihu.com/python-kivy

记笔记的人：[shouldsee](https://www.zhihu.com/people/shouldsee/activities)


### 隐变量模型

在Chap10\{ref:chap-10}我们呈现了用图模型定义高维联合概率分布的操作。其基本理念在两个变量之间加入连边来模拟它们之间的依赖性(严格来讲图模型规定的是条件独立性，但你应该懂我的意思)。

当然我们可以从另一个角度来假设观测变量的关联性来自于一个隐藏的，共同的“原因”。这种含有隐藏变量的模型就叫做**隐变量模型**(LVM:Latent variable models)。在本章节中，我们会看到这些模型会比没有隐变量的模型更难拟合。然而，隐变量模型有两大优点:

第一，隐变量模型一般来说比那些显式地处理关联性的模型有更少的参数。这在图11.1\ref{fig:11.1}中可以看出：如果所有的节点（包括H）都是二值，并且所有的条件概率分布都是查表的，那么左边的那个模型有17个自由参数，儿右边的那个模型有59个自由参数。

第二，隐变量模型中的隐藏变量可以起**瓶颈**的作用，并将原来的数据转化成某种压缩过的表示。我们接下来会看到这其实是无监督学习的基础。图11.2(\ref{fig：11.2})画出了一些可以作此用途的通用隐变量模型结构。通常来讲我们有 $L$ 个隐变量， $z_{i1},\dots,z_{iL}$ 以及 $D$ 个显变量 $x_{i1},\dots,x_{iD}$ ，并且 $D\ll L$。如果有 $L>1$，那么每个观测变量都是多个因变量共同作用的结果，于是就形成了一个多对多的映射。如果 $L=1$， 我们就只有一个隐变量。 此时一般使用一个离散的 $z_i$ ,也就形成了一个一对多的映射。 当然也存在多对一的映射，也就是多个因子或者原因争相解释一个观测变量。 这样的模型也是概率性矩阵分解的基础， 我们在Sec27.6.2\ref{sec:27.6.2}中会加以讨论。 最后，也存在一对一的的映射 $z_i \rightarrow x_i$。 如果我们允许 $z_i$ 和/或 $x_i$ 取矢量值，这个形式就可以囊括其他的几个形式。 通过取不同的似然函数 $p(x_i \gvn z_i)$ 和先验函数 $p(z_i)$　，我们可以得到一系列模型，见表Table11.1\ref{tab:11.1}。

\begin{table}

| $p(x_i \gvn z_i)$ | $p(z_i)$| 名称 | 章节 |
|:-----------------:| ------------ | ----------- |------ |
|　多维正态分布 | 范畴分布 | 高斯混合模型 | 11.2.1\ref{sec:11.2.1} |
|　范畴分布之积　|　范畴分布　|　多项混合分布 | 11.2.2\ref{sec:11.2.2} |
|　高斯分布之积|高斯分布之积|因子分析/概率性ＰＣＡ | 12.1.5\ref{sec:12.1.5}|
|　高斯分布之积|拉普拉斯分布之积|　稀疏编码/概率性ＩＣＡ | 12.6\ref{sec:12.6} |
|　范畴分布之积|　高斯分布之积　| 多项分布ＰＣＡ(？前面两个貌似反了) | 27.2.3\ref{sec:27.2.3} |
| 范畴分布之积| 狄利克雷分布| 隐狄利克雷分配(LDA)| 27.3\ref{sec:27.3} |
|含噪或函数之积| 伯努利函数之积 | BN20/QMR | 10.2.3\ref{sec:10.2.3}|
|伯努利函数之积 |伯努利函数之积 | S形函数信念网络 |
\caption{本表总结了一些比较火爆的有向隐变量模型。此处一个“范畴分布之积”的似然函数指一个可以因子分解的分布$\prod_j \text{Cat}(x_{ij}\gvn z_i)$。“高斯分布之积”指可分解的分布 $\prod_j \mathcal{N} (x_{ij} \gvn z_i )$。 "PCA" 指主成分分析(Principal Components Analysis); “ICA”指独立成分分析 (Independent Components Analysis)
\label{tab:11.1}
\end{table}

## 混合模型

最简单的隐变量模型具有隐变量 $z_i\in\{1,\dots,K\}$， 表示一个离散的隐藏状态，对此我们采用一个范畴函数作为先验 $p(z_i) = \text{Cat}(\pi)$。 对似然函数，我们采用 $p(x_i\gvn z_i =k ) =p_k(x_i)$，其中 $p_k$ 是观测值的第k个基底分布，并且可以取任意形式。这个模型总体上讲是一个混合模型，因为 $K$ 个基底分布通过以下的形式混合：

$$
p (x_i \gvn \theta) = \sum^K_{k=1}{ \pi_k p_k(x_i \gvn \theta) } \tag{11.1}
$$

这是所有 $p_k$ 的一个**凸组合**，因为我们取的是一个加权和，并且权重 $\pi_k$ 满足 $0 \le \pi_k \le 1$　并且有 $\sum_k^{K}\pi_k =1$。 我们接下来给出一些例子。

### 高斯混合模型

最常用的混合模型是**高斯混合模型**(MOG：Mixture of Gaussians, GMM: Gaussian Mixture Model)。在这个模型中，每个基底分布都是一个多变量高斯分布,并有均值 $\mu_k$ 和协方差矩阵 $\Sigma_k$。因而其似然函数有以下形式

$$
p(x_i \gvn \theta) = \sum_{k=1}^K \pi_k \mathcal{N}(x_i \gvn \mu_k,\Sigma_k) 
\tag{11.2}
$$

图Fig11.3\ref{fig:11.3}展示了3个高斯混合模型的联合概率分布2D图。 每一个混合成分都由不同的椭圆形轮廓线显示。当混合成分的数量足够大，这个高斯混合模型就可以近似任意一个定义在 $\mathbb{R}^D$ 上的概率密度。

### 范畴/多努利混合模型（mixture of multinoullis）



# MLAPP 读书笔记 

## 11 混合模型和期望最大化算法

> A Chinese Notes of MLAPP，MLAPP 中文笔记项目 
https://zhuanlan.zhihu.com/python-kivy

记笔记的人：[shouldsee](https://www.zhihu.com/people/shouldsee/activities)


### 隐变量模型

在Chap10\{ref:chap-10}我们呈现了用图模型定义高维联合概率分布的操作。其基本理念在两个变量之间加入连边来模拟它们之间的依赖性(严格来讲图模型规定的是条件独立性，但你应该懂我的意思)。

当然我们可以从另一个角度来假设观测变量的关联性来自于一个隐藏的，共同的“原因”。这种含有隐藏变量的模型就叫做**隐变量模型**(LVM:Latent variable models)。在本章节中，我们会看到这些模型会比没有隐变量的模型更难拟合。然而，隐变量模型有两大优点:

第一，隐变量模型一般来说比那些显式地处理关联性的模型有更少的参数。这在图11.1\ref{fig:11.1}中可以看出：如果所有的节点（包括H）都是二值，并且所有的条件概率分布都是查表的，那么左边的那个模型有17个自由参数，儿右边的那个模型有59个自由参数。

第二，隐变量模型中的隐藏变量可以起**瓶颈**的作用，并将原来的数据转化成某种压缩过的表示。我们接下来会看到这其实是无监督学习的基础。图11.2(\ref{fig：11.2})画出了一些可以作此用途的通用隐变量模型结构。通常来讲我们有 $L$ 个隐变量， $z_{i1},\dots,z_{iL}$ 以及 $D$ 个显变量 $x_{i1},\dots,x_{iD}$ ，并且 $D\ll L$。如果有 $L>1$，那么每个观测变量都是多个因变量共同作用的结果，于是就形成了一个多对多的映射。如果 $L=1$， 我们就只有一个隐变量。 此时一般使用一个离散的 $z_i$ ,也就形成了一个一对多的映射。 当然也存在多对一的映射，也就是多个因子或者原因争相解释一个观测变量。 这样的模型也是概率性矩阵分解的基础， 我们在Sec27.6.2\ref{sec:27.6.2}中会加以讨论。 最后，也存在一对一的的映射 $z_i \rightarrow x_i$。 如果我们允许 $z_i$ 和/或 $x_i$ 取矢量值，这个形式就可以囊括其他的几个形式。 通过取不同的似然函数 $p(x_i \gvn z_i)$ 和先验函数 $p(z_i)$　，我们可以得到一系列模型，见表Table11.1\ref{tab:11.1}。

\begin{table}

| $p(x_i \gvn z_i)$ | $p(z_i)$| 名称 | 章节 |
|:-----------------:| ------------ | ----------- |------ |
|　多维正态分布 | 范畴分布 | 高斯混合模型 | 11.2.1\ref{sec:11.2.1} |
|　范畴分布之积　|　范畴分布　|　多项混合分布 | 11.2.2\ref{sec:11.2.2} |
|　高斯分布之积|高斯分布之积|因子分析/概率性ＰＣＡ | 12.1.5\ref{sec:12.1.5}|
|　高斯分布之积|拉普拉斯分布之积|　稀疏编码/概率性ＩＣＡ | 12.6\ref{sec:12.6} |
|　范畴分布之积|　高斯分布之积　| 多项分布ＰＣＡ(？前面两个貌似反了) | 27.2.3\ref{sec:27.2.3} |
| 范畴分布之积| 狄利克雷分布| 隐狄利克雷分配(LDA)| 27.3\ref{sec:27.3} |
|含噪或函数之积| 伯努利函数之积 | BN20/QMR | 10.2.3\ref{sec:10.2.3}|
|伯努利函数之积 |伯努利函数之积 | S形函数信念网络 |
\caption{本表总结了一些比较火爆的有向隐变量模型。此处一个“范畴分布之积”的似然函数指一个可以因子分解的分布$\prod_j \text{Cat}(x_{ij}\gvn z_i)$。“高斯分布之积”指可分解的分布 $\prod_j \mathcal{N} (x_{ij} \gvn z_i )$。 "PCA" 指主成分分析(Principal Components Analysis); “ICA”指独立成分分析 (Independent Components Analysis)
\label{tab:11.1}
\end{table}

## 混合模型

最简单的隐变量模型具有隐变量 $z_i\in\{1,\dots,K\}$， 表示一个离散的隐藏状态，对此我们采用一个范畴函数作为先验 $p(z_i) = \text{Cat}(\pi)$。 对似然函数，我们采用 $p(x_i\gvn z_i =k ) =p_k(x_i)$，其中 $p_k$ 是观测值的第k个基底分布，并且可以取任意形式。这个模型总体上讲是一个混合模型，因为 $K$ 个基底分布通过以下的形式混合：

$$
p (x_i \gvn \theta) = \sum^K_{k=1}{ \pi_k p_k(x_i \gvn \theta) } \tag{11.1}
$$

这是所有 $p_k$ 的一个**凸组合**，因为我们取的是一个加权和，并且权重 $\pi_k$ 满足 $0 \le \pi_k \le 1$　并且有 $\sum_k^{K}\pi_k =1$。 我们接下来给出一些例子。

### 高斯混合模型

最常用的混合模型是**高斯混合模型**(MOG：Mixture of Gaussians, GMM: Gaussian Mixture Model)。在这个模型中，每个基底分布都是一个多变量高斯分布,并有均值 $\mu_k$ 和协方差矩阵 $\Sigma_k$。因而其似然函数有以下形式

$$
p(x_i \gvn \theta) = \sum_{k=1}^K \pi_k \mathcal{N}(x_i \gvn \mu_k,\Sigma_k) 
\tag{11.2}
$$

图Fig11.3\ref{fig:11.3}展示了3个高斯混合模型的联合概率分布2D图。 每一个混合成分都由不同的椭圆形轮廓线显示。当混合成分的数量足够大，这个高斯混合模型就可以近似任意一个定义在 $\mathbb{R}^D$ 上的概率密度。

### 范畴/多努利混合模型（mixture of multinoullis）

混合模型可以近似很多



 
在本章节中，我们考虑一个更加一般的、基于变分推断的确定性近似推断算法(@Jordan1998，@Jakkola&Jordan2000,@Jaakola2001，@Wainwright&Jordan2008a)。其基本理念是从易处理的分布族中选取一个近似分布$q(x)$，然后设法让这个近似分布尽可能地接近真正的后验$p^*(x)\triangleq p(x|D)$。这样以来原本的推断问题就简化成了一个最优化问题。通过放宽限制或者对目标函数再次作近似，我们可以在精度和速度之间寻找平衡。因此变分推断可以用最大后验估计（MAP）算法的速度实现贝叶斯方法的统计学优势。





### 21.2 变分推断法

假设$p^*(x)$使我们的真实却难以处理的分布，而$q(x)$是某个便于处理的近似分布，比如说一个多维高斯分布或者因子分解过的分布。我们假设$q$具有一些可以自由参数，并且我们可以通过优化这些参数使得$q$更加像$p^*$。我们显然可以最小化损失函数KL散度:

$$
\KLDiv)(p^*||q) = \sum_{x} p^*（x） \log \frac{p^*(x)}{q(x)}  ~~\tag{21.1}\label{eqn:21.1}
$$

但是，这玩意非常难算，因为在分布$p^*$上求期望根据题设是难以处理的。一个自然的替代选项是最小化逆KL散度：

$$
\KLDiv)(q||p^*) = \sum_{x} q(x) \log \frac{q(x)}{p^*(x)}  ~~\tag{21.2}\label{eqn:21.2}
$$

这个目标函数最大的优势是在分布$q$上的期望是便于计算的(通过选取适当形式的$q$)。我们会在Sec21.2.2\ref{sec:21.2.2}中讨论这两个目标函数的区别。

不幸的是，公式Eqn21.2\ref{eqn:21.2}仍然没有看起来那么好算，因为即便逐点计算$p(x|D)$也是很困难的，因为有一个正规化常数$Z=p(D)$是难以处理的。但是呢，一般来说未正规化的分布$\tilde{p}(x)\triangleq p(x,D)=p^*(x)|$是很好计算的。所以我们将目标函数改为如下:

$$
J（q） = \KLDiv(q||\tilde{p})
~~\tag{21.3}\label{eqn:21.3}
$$

当然这个写法有点滥用记号的意思，因为$\tilde{p}$严格意义上讲并不是一个概率分布。不过无所谓，让我们带入KL散度的定义：

\begin{align}
J(q) &= \sum_{x} q(x) \log \frac{q(x)}{\tilde{p}(x)}
~~\tag{21.4}\label{eqn:21.4}
\\
&= \sum_{x} q(x) \log \frac{q(x)}{Z p ^ * (x)}
~~\tag{21.5}\label{eqn:21.5}
\\ &= \sum_{x} q(x) \log \frac{q(x)}{p^*(x)} - \log Z
~~\tag{21.6}\label{eqn:21.6}
\\
&= \KLDiv(q||p^*) - \log Z
~~\tag{21.7}\label{eqn:21.7}
\end{align}

因为$Z$是一个常数，所以最小化$J(q)$的同时我们也就达到了迫使 $q$ 趋近 $p^*$ 的目的。

因为KL散度总是非负的，可以看出$J(q)$是负对数似然(NLL:Negative Log Likelihood)的上界:

$$
J(q) = \KLDiv(q||p^*) - \log Z \ge -\log Z = -\log p（D）
~~\tag{21.8}\label{eqn:21.8}
$$

换句话说，我们可以尝试最大化如下被称作能量泛函的量(@Koller&Friedman2009)。它同时也是数据似然度的下界：

$$
L(q) \triangleq -J(q) = - \KLDiv(q||p^*) + \log Z \le \log Z = \log p(D)
~~\tag{21.9}\label{eqn:21.9}
$$

因为这个界在$q=p^*$时是紧的，可以看出变分推断法和EM算法联系之紧密(见Sec11.4.7\ref{sec:11.4.7})。


#### 21.2.1 变分目标函数的其他意义

前述的目标函数还有几种同样深刻的写法。其中一种如下：

$$
J(q) = \bE_q[\log q(x) ] + \bE [-\log \tilde{p}(x)] = - \mathbb{H}(q) + \bE_q [E(x)] 
~~\tag{21.10}\label{eqn:21.10}
$$

也就是能量的期望（因为$E(x)=-\log \tilde { p} (x)$）减去系统的熵。在统计物理里，$J(q)$被称为变分自由能，或者也叫亥姆霍兹自由能。

另一写法如下:

\begin{align}
J(q) 
&= \bE_q{ [\log q(x) - \log p(x) p (D|x) ] }
~~\tag{21.11}\label{eqn:21.11}
\\ 
&= \bE_q{ [\log q(x) - \log p(x) - \log p (D|x) ] }
~~\tag{21.12}\label{eqn:21.12}
\\ 
&= \bE_q [ -\log p(D|x)  ] + \KLDiv (q(x)||p(x)) 
~~\tag{21.13}\label{eqn:21.13}
\end{align}

也就是负对数似然的期望加上一个表示后验分布到确切的先验距离的惩罚项。

我们还可以从信息论的角度理解(也叫作[bits-back论述](http://users.ics.aalto.fi/harri/thesis/valpola_thesis/node30.html)，具体见(@Hinton&Camp1993,@Honkela&Valpola2004))。

#### 21.2.2 前向(Forward)还是后向(Reverse)KL散度？

因为KL散度是非对称的，对 $q$ 最小化 $\KLDiv(q||p)$ 和 $\KLDiv(p||q)$ 会给出不同的结果。 接下来我们讨论一下两者的异同。

首先考虑后向KL， $\KLDiv(q||p)$，也称**I-投影**或**信息投影**。根据定义有

$$
\KLDiv(q||p) = \sum_x q(x) \ln {q(x) \over p(x)}
~~\tag{21.14}\label{eqn:21.14}
$$

这个量在 $p(x)=0$ 且 $q(x)>0$ 是无穷的。 因此如果 $p(x)=0$ 就必须要有 $q(x) = 0$。 因此后向KL被称作是**迫零的**，而且近似分布 $q$ 常常会欠估计 $p$ 的支撑集。

接下来考虑前向KL,也称**M-投影**或者**矩投影**

$$
\KLDiv(p||q) = \sum_x p(x) \ln {p(x) \over q(x)}
~~\tag{21.15}\label{eqn:21.15}
$$

这个量在 $q(x)=0$ 且 $p(x)>0$ 是无穷的。 因此如果 $p(x)>0$ 就必须要有 $q(x) > 0$。因此前向KL被称作**避零的**，而且近似分布 $p$ 常常会过估计 $p$ 的支撑集。

两者的区别请见图Fig21.1\ref{fig:21.1}。可以发现当真实分布 $p$ 是多模态的时候，前向KL是一个很差的选择(此处假设使用了一个单模态的 $q$),因为它给出的后验的众数/平均数会落在一个低密度的区域，恰恰落在两个模态的峰值之间。在这个情况下，后向KL不仅更便于计算，也具有更好的统计性质。

\label{fig:21.1}

\label{fig:21.2}

另一个区别显示在图Fig\ref{fig:21.2}中。此处的真实分布是一个拉长的二维高斯分布而近似分布是两个一维高斯的乘积。换句话说 $p(x) = \mathcal{N}(x|\mu,\Lambda^{-1})$，且

$$
\mu = \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}
,
\Lambda = \begin{pmatrix} 
\Lambda_{11} & \Lambda_{12}
\\   \Lambda_{21} & \Lambda_{22}
\end{pmatrix}
$$

在图Fig21.2a\ref{fig:21.2a}中我们给出了最小化后向KL $\KLDiv(q||p)$ 的结果。在这个例子中，我们可以证明有如下解

\begin{align}
q(x) &= \mathcal{N}(x_1|m_1,\Lambda_{11}^{-1})
\mathcal{N}(x_2|m_2,\Lambda_{22}^{-1})
~~\tag{21.17}\label{eqn:21.17}
\\ 
m1 &=  \mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (m_2 - \mu_2)
~~\tag{21.18}\label{eqn:21.18}
\\ 
m2 &=  \mu_2 - \Lambda_{22}^{-1} \Lambda_{21} (m_1 - \mu_1)
~~\tag{21.19}\label{eqn:21.19}
\end{align}


图Fig21.2a\ref{fig:21.2a}的结果显示我们正确地估计了平均值，但是这个近似太紧凑了：它的方差是由 $p$ 的方差最小的那个方向所决定的。事实上，通常情况下（当然也有特例 @Turner2008）在 $q$ 是乘积分布时最小化 $\KLDiv(q||p)$ 会给出一个过于置信的近似。

图Fig21.2b\ref{fig:21.2b}给出了最小化 $\KLDiv(p||q)$ 的结果。在习题Exer21.7\ref{exer:21.7}我们已经证明对一个乘积分布最小化正向KL给出的最优解正好是其真实边际分布的乘积，也就是说有

$$
q(x) = \mathcal{N}(x_1 | \mu_1,\Lambda_{11}^{-1})
\mathcal{N}(x_2 | \mu_2,\Lambda_{22}^{-1})
~~\tag{21.20}\label{eqn:21.20}
$$

图Fig21.2b\ref{fig:21.2b}显示出这个估计是过泛的，因为它过估计了 $p$ 的支撑集。

在本章的剩余部分，以及本书接下来的大部，我们会专注于最小化后向KL $\KLDiv (q||p)$。在Sec22.5\ref{sec:22.5}对期望传播的阐述中，我们会探讨前向KL $\KLDiv{p||q}$ 在局部的优化。

#### 21.2.3 另外的一些相关的度量

通过引入参数$\alpha \in \mathbb{R}$我们可以定义如下**alpha散度**：

$$
D_{\alpha}( p || q ) \triangleq \frac{4}{1-\alpha^2} 
\left( 1 - \int p(x) ^{(1+\alpha)/2} q(x) ^{(1-\alpha)/2} dx
\right)
~~\tag{21.21}\label{eqn:21.21}
$$

这个量满足 $D_\alpha (p || q) \iff p=q$，但是它们显然也是不对称的，因而不是一个度规。 $\KLDiv(p||q)$ 对应极限 $\alpha \rightarrow 1$ ，而 $\KLDiv(q||p)$ 对应极限 $\alpha \rightarrow -1$。当 $\alpha=0$，我们取得一个和海灵格距离线性相关的对称的散度，定义如下

$$
D_H(p||q) \triangleq \int \left( p(x)^{1\over2} - q(x) ^{1\over 2}\right)^2
~~\tag{21.22}\label{eqn:21.22}
$$

注意到 $\sqrt{D_H(p||q)}$ 是一个有效的距离度规。也就是说，它对称非负且满足三角不等式，详见(@Minka2005)。
